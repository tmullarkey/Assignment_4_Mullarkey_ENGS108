{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thayer-ENGS108/Assignment_4_Fall2022/blob/main/assignment_4_Fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd0qJjGWPDEY"
      },
      "source": [
        "# **ENGS 108 Fall 2022 Assignment 4**\n",
        "\n",
        "*Due October 11, 2022 at 11:59PM on Github*\n",
        "\n",
        "**Instructors:** George Cybenko\n",
        "\n",
        "**TAs:** Chase Yakaboski and Clement Nyanhongo\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Rules and Requirements**\n",
        "\n",
        "\n",
        "1.   You are only allowed to use Python packages that are explicity imported in \n",
        "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
        "  *   [numpy](https://numpy.org/doc/stable/)\n",
        "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
        "  *   [matplotlib](https://matplotlib.org/)\n",
        "  *   [tensorflow](https://www.tensorflow.org/)\n",
        "\n",
        "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
        "\n",
        "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD5eAz9acxe9"
      },
      "outputs": [],
      "source": [
        "''' Import Statements '''\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcqjf9VXHfF"
      },
      "source": [
        "# Data Loading\n",
        "In this assignment we will be using the Yoga Postures Dataset from [Kaggle](https://www.kaggle.com/datasets/tr1gg3rtrash/yoga-posture-dataset) and will use the following code blocks to download the dataset directly to your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZ4gJ5zuZjP"
      },
      "source": [
        "## Creating a Kaggle API Token\n",
        "First we will need to download an API token from Kaggle in order to download the dataset, so our first step is to create a Kaggle account if you don't already have one.\n",
        "1. Create a Kaggle account by following the sign up instructions [here](https://www.kaggle.com/).\n",
        "2. Log into your Kaggle account and click your account icon on the upper righthand side. \n",
        "3. Then select **Account** from the dropdown/sidebar menu.\n",
        "4. Scroll down to the **API** section and select **Create New API Token**.\n",
        "5. This will download a JSON file called kaggle.json to your Downloads folder on your computer.\n",
        "6. Now run the following code block and when the **Browse** button appears, click it and select that kaggle.json file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSQSg2HkXaHF"
      },
      "outputs": [],
      "source": [
        "# Run this code block after creating a Kaggle API token as instructed above.\n",
        "! pip install -q kaggle\n",
        "from google.colab import files \n",
        "\n",
        "# Will ask you to upload kaggle.json file and remove any old ones.\n",
        "if os.path.exists('kaggle.json'):\n",
        "  os.remove('kaggle.json')\n",
        "files.upload()\n",
        "\n",
        "# Will create the appropriate directory structure\n",
        "if not os.path.exists('/root/.kaggle'):\n",
        "  ! mkdir ~/.kaggle \n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOhg0Rxx1HB"
      },
      "source": [
        "## Downloading the Dataset\n",
        "\n",
        "7. Now we have downloaded our Kaggle credentials we can now download the Yoga Postures Dataset (or any other Kaggle dataset for that matter) directly into our Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r2Nya5yV3hW"
      },
      "outputs": [],
      "source": [
        "# Will ensure a train directory exists in your home and will create if it doesn't exist.\n",
        "if not os.path.exists('train'):\n",
        "  os.mkdir('train')\n",
        "# Will download the yoga postures dataset zip file if it does not exist in the train directory.\n",
        "if not os.path.exists('train/yoga-posture-dataset.zip'):\n",
        "  ! kaggle datasets download -p train tr1gg3rtrash/yoga-posture-dataset\n",
        "# Will check to see if the yoga postures zip file has been unzipped and will unzip the file if not.\n",
        "if not os.path.exists('train/Poses.json'):\n",
        "  ! unzip train/yoga-posture-dataset.zip -d train\n",
        "\n",
        "# To remove train folder and redownload uncomment the code below\n",
        "#!rm -rf train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyYOK_sA1rxB"
      },
      "source": [
        "## Viewing the Dataset Metadata\n",
        "Now let's inspect the dataset metadata, i.e. the information about each of the training examples, which is located in the Poses.json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMVBxxYS1ra1"
      },
      "outputs": [],
      "source": [
        "# Let's load the yoga postures metadata\n",
        "import json\n",
        "with open('train/Poses.json', 'r') as metafile:\n",
        "  metadata = json.load(metafile)\n",
        "metadata = pd.DataFrame.from_dict(metadata['Poses'])\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mty9AB1bd5hH"
      },
      "source": [
        "## Pre-Problem: Segmenting Out the Test Data and Copying to Google Drive\n",
        "A part of this homework will be a competition. Included in your assignment github repo, there is a file called **test_images.csv**. This file holds all of the image paths that you are **NOT** allowed to use during *training*. Therefore, what we are going to do in this pre-problem is remove all the testing images from your train data folder. *Hint:* You naturally are still allowed/should use the test data to evaluate your model moving foward, you're just not allowed to use these images during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8V1hjp0rhOw"
      },
      "source": [
        "### Task 1\n",
        "Using the code available in the Data loading section as a reference, write a command using the \"!\" colab operator to create a new directory called **test**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX8v3DzArOf6"
      },
      "outputs": [],
      "source": [
        "#TODO: Write one line of code to make a new directory called test (You might need to refresh your files view to see the change)\n",
        "! mkdir test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Ln5qixfHwt"
      },
      "source": [
        "### Task 2\n",
        "Upload your **test_images.csv** file by running the following code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCY6MmC-6HhX"
      },
      "outputs": [],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFo9c6U2--hm"
      },
      "source": [
        "### Task 3\n",
        "Now let's write some code to copy over all these test images into the test folder we created as well as removing them from the train folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt00afQK_Ih7"
      },
      "outputs": [],
      "source": [
        "# First let's load the test_images.csv file\n",
        "with open('test_images.csv', 'r') as f_:\n",
        "  reader = csv.reader(f_)\n",
        "  test_image_paths = []\n",
        "  for row in reader:\n",
        "    test_image_paths.append(row[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYX2AlPPsIot"
      },
      "outputs": [],
      "source": [
        "# TODO: Now using these test image paths, copy over these files to the test directory and remove them from the train directory\n",
        "for test_path in test_image_paths:\n",
        "  # Do something... (hint see cp and rm commands using the colab terminal, i.e. the \"!\" operator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQSiUHBUv_6Q"
      },
      "source": [
        "### Task 4\n",
        "Now that we have our test and training sets setup, let's copy them over to our Google Drive so that we don't have to run this pipeline again.\n",
        "\n",
        "1. Mount your Google Drive in colab in the standard way by running the following code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZcnh30iwVVa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0MK7c3lwhz8"
      },
      "source": [
        "2. Now run the following command to copy over the training and test data to your desired location. Remember this location so you don't have to run this data loading section everytime you want to work on your homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUnorFMiwWIb"
      },
      "outputs": [],
      "source": [
        "# DESIRED GOOGLE DRIVE DIRECTORY (CHANGE ME)\n",
        "DESIRED_GOOGLE_DRIVE_DIR = '/content/drive/MyDrive/data/yoga-poses'\n",
        "\n",
        "# Makes the desired directory. If you get an error then the directory likely already exists, so comment out this line.\n",
        "! mkdir -p $DESIRED_GOOGLE_DRIVE_DIR\n",
        "\n",
        "# Command to copy over all test and train data to desired google drive path\n",
        "! cp -r train $DESIRED_GOOGLE_DRIVE_DIR\n",
        "! cp -r test $DESIRED_GOOGLE_DRIVE_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPvUebuqx7Zv"
      },
      "source": [
        "# Problem 1: Support Vector Machine Classification\n",
        "In this problem, you will be building support vector machines for some cool classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcidjoN9yfDn"
      },
      "source": [
        "## Task 1: Synthetic Data\n",
        " In this part we will be exploring the *circles* dataset. This dataset is avialable as a pickle file labeled *circles.pk* in your assignment repository under datasets. In this dataset you will have an $X$ array of 2 dimensional samples of the form $(x_1, x_2)$ and a $y$ array of each samples associated label. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udSAfh76ypCS"
      },
      "source": [
        "### Part A\n",
        "Upload this dataset to your Google Drive and load the dataset into memory using the pickle module (*Hint: Refer to previous assignments to figure out how to do this*). Go through the circles dataset and create a scatterplot of the circles data using the y label as each sample's color to designate their respective class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbHlzNfazdB1"
      },
      "outputs": [],
      "source": [
        "# TODO: Change me\n",
        "CIRCLES_DATASET_PATH = '/content/drive/MyDrive/data/circles/circles.pk'\n",
        "\n",
        "# TODO: Load the dataset using pickle load \n",
        "with open(CIRCLES_DATASET_PATH, 'rb') as circles_file:\n",
        "  # TODO: Load here...\n",
        "  # circles = ??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwLr5n7I0kUD"
      },
      "outputs": [],
      "source": [
        "X, y = circles\n",
        "# TODO: Graph the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaNdT9dO1GNM"
      },
      "source": [
        "### Part B\n",
        "Is this dataset linearly seperable? Explain why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm2CzCTG1JSr"
      },
      "source": [
        "**TODO:** Your answer should go here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFK8qmFg1o8V"
      },
      "source": [
        "### Part C\n",
        "Can you think of a transformation that can be applied to this dataset that could make it linearly seperable? If so, define what these transformation function(s) might look like, and if not explain why. *Hint: Think of a higher dimensional space.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia1Jhy_L1uax"
      },
      "source": [
        "**TODO:** Your answer should go here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC5ZUiZD1-ew"
      },
      "source": [
        "### Part D\n",
        "If you where able to find a transformation in (Part C), create a suitable graph showing that the dataset is linearly seperable in this new feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY_vnGqB2FxG"
      },
      "outputs": [],
      "source": [
        "#TODO: Your code goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXBD02Z72XTn"
      },
      "source": [
        "## Task 2: Yoga Pose Classification\n",
        "What we accomplished in Part 1 is known as the kernel trick for SVMs. Now let's focus on how we can use this idea to accomplish non-linear classification on a real world dataset. In this next part and throughout the remainder of the assignment we will be using a yoga postures dataset. These images are PNG and JPEG images with many pixels. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgKTi7eS2ftd"
      },
      "source": [
        "### Part A\n",
        "You have been given a number of code skeletons throughout the course all of which load and preprocess the data for you. In this excerise tho, we will be doing the data loading manually as it is an important skill to learn. Write some code that will walk through the *Yoga Postures Dataset* directory structure and build a single large numpy array (Make sure to resize the image to something like (28, 28) or (32, 32) for the SVM and at minimum (71, 71) for the deep learning tasks) *Hint: You have been provided with a basic skeleton, study the operations of the code and finish the script.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TqNTGVz3CWF"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# TODO: Change Me\n",
        "PATH_TO_YOGA_TRAIN_DATA = '/content/drive/MyDrive/data/yoga-poses/train'\n",
        "PATH_TO_YOGA_TEST_DATA = '/content/drive/MyDrive/data/yoga-poses/test'\n",
        "\n",
        "# The dimensions of the resized image\n",
        "RESIZE = (71,71)\n",
        "# A map from integer ids to yoga pose categories (strings)\n",
        "pose_map = {}\n",
        "# The data lists that we will be filling in.\n",
        "X, y = [], []\n",
        "\n",
        "# Let's start our for loop (Just using tqdm to give us a pretty progress bar).\n",
        "class_id = 0\n",
        "for subfold in tqdm.tqdm(os.listdir(PATH_TO_YOGA_TRAIN_DATA), desc='Processing training images', leave=False):                                                                             \n",
        "  if os.path.isdir(os.path.join(PATH_TO_YOGA_TRAIN_DATA, subfold)):\n",
        "    # We have found image class folder so let's extract all example data\n",
        "    pose_map[class_id] = subfold\n",
        "    for img_name in os.listdir(os.path.join(PATH_TO_YOGA_TRAIN_DATA, subfold)):\n",
        "      #TODO: Preprocess and load image\n",
        "      #HINT: PNGs will have 4 channels and JPEGs will have 3 (THIS WILL CAUSE ISSUES!).\n",
        "      # Try to figure out what to do with this 4th channel. It could be as simple as deleting it...\n",
        "    class_id += 1\n",
        "\n",
        "X_train = np.array(X)\n",
        "y_train = np.array(y)\n",
        "\n",
        "# Let's load in the training data as well using the same methodology but with the pose_map built during the training data collection. \n",
        "pose_map_reverse = {pose: class_id for class_id, pose in pose_map.items()}\n",
        "X, y = [], []\n",
        "for subfold in tqdm.tqdm(os.listdir(PATH_TO_YOGA_TEST_DATA), desc='Processing test images', leave=False):                                                                             \n",
        "  if os.path.isdir(os.path.join(PATH_TO_YOGA_TEST_DATA, subfold)):\n",
        "    # We have found image class folder so let's extract all example data\n",
        "    class_id = pose_map_reverse[subfold]\n",
        "    for img_name in os.listdir(os.path.join(PATH_TO_YOGA_TEST_DATA, subfold)):\n",
        "      #TODO: Preprocess and load image\n",
        "\n",
        "X_test = np.array(X)\n",
        "y_test = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39zc6f-9UDr"
      },
      "source": [
        "### Part B\n",
        "Implement a SVM classifer using sklearn and report your classification results on the testing dataset. Make sure to flatten your X data from the three channels before passing into the SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fPEAkUB9jwh"
      },
      "outputs": [],
      "source": [
        "#TODO: Build and fit your SVM classifier and print your classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq3fmX8K_l_N"
      },
      "source": [
        "# Problem 2: Introduction to TensorFlow\n",
        "In this problem, we will start working in tensorflow to build deep learning systems starting with fully-connected (FC) and convolutional (CNN) neural networks. We will focus on using the yoga postures dataset, ending this problem with a small class competition that may come with a prize..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNX1muh_7IA"
      },
      "source": [
        "## Task 1: A Fully-Connected Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCkRDTbIA4vH"
      },
      "source": [
        "### Part A\n",
        "Using the yoga postures dataset, build a [tensorflow Data Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that is shuffled with a batch size of 10. *Hint: We did this in class.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAUkPd1lAFHh"
      },
      "outputs": [],
      "source": [
        "#TODO: Your code goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlLZH8fsA9NF"
      },
      "source": [
        "### Part B\n",
        "Build a two layer fully connected neural network of any size with a ReLu activation function and a final softmax layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RXomqTaBLHw"
      },
      "outputs": [],
      "source": [
        "#TODO: Your code goes here. Make sure to add a Flatten layer first with the correct input shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR3MLamSBX3E"
      },
      "source": [
        "### Part C\n",
        "Compile your model with an appropriate loss function and optimizer. Briefly describe your choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHRuyQk9Bddp"
      },
      "outputs": [],
      "source": [
        "#TODO: Your code goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0V_gmYBsc-"
      },
      "source": [
        "### Part D\n",
        "Train your model on the yoga pose training dataset. And report your accuracy on the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmQcGH1TBxoP"
      },
      "outputs": [],
      "source": [
        "#TODO: Your code goes here. Hint: It's ok if the accuracy is not the best..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMw_u7qsFWCf"
      },
      "source": [
        "## Task 2: A Convolutional Neural Network\n",
        "CNNs perform notoriously well on image classification tasks, so we will study a simple implementation of this network and let you guys compete to see who finds the best architecture. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OScsuOosFzqP"
      },
      "source": [
        "### Part A: Vanilla CNN\n",
        "Build, compile, and fit a CNN with the following network structure:\n",
        "* A total of 3 convolutional segments consisting of:\n",
        "  - A convolutional layer with \"valid\" padding and Relu activation and your choice of kernel size, number of filters and strides,\n",
        "  - An average pooling layer,\n",
        "* Then a fully connected layer with 128 neurons with ReLu activations.\n",
        "* Then a fully connected layer with 64 neurons and ReLu activations.\n",
        "* And ending with a softmax output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsWa-EPcHewF"
      },
      "outputs": [],
      "source": [
        "#TODO: Build, compile and train this network on the yoga pose training data. Maybe this one will be better..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B: Transfer Learning\n",
        "Now we are going to steal from a state-of-the-art [Xception](https://arxiv.org/abs/1610.02357) model. Luckily, tensorflow has this directly available to us and we are going to use transfer learning to tune this SoA model to our dataset. Therefore, run the following code block to build our base Xception model:"
      ],
      "metadata": {
        "id": "ImJlwhxDcm1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I'll setup your base model for you.\n",
        "from tensorflow.keras.applications import Xception\n",
        "\n",
        "base_model = Xception(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(71,71,3)\n",
        ")\n",
        "base_model.trainable=False"
      ],
      "metadata": {
        "id": "pItcNDoZQz2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a new model with this *base_model* as our first layer followed by:\n",
        "* Global average pooling layer,\n",
        "* A Dense layer with 256 neurons and ReLu activations,\n",
        "* A 40% Dropout layer,\n",
        "* A softmax output layer.\n",
        "\n",
        "Then fit this model on 30 epochs."
      ],
      "metadata": {
        "id": "p6pRc41Gds2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Finish the model\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    #TODO: Add other layers\n",
        "])\n",
        "\n",
        "#TODO: Compile model"
      ],
      "metadata": {
        "id": "qGseIBQ9efhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Fit the model on 30 epochs"
      ],
      "metadata": {
        "id": "TesGjF0ser0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part C: Build your Own Model\n",
        "So know that you know the range of accuracies between poorly built models and state-of-the-art models like Xception, can you improve on them? Attempt to build your own deep neural network architecture and see if you can do better than the base CNN model presented above or better yet the Xception model we transfer learned. You may use any architecture or configuration you like."
      ],
      "metadata": {
        "id": "hrER79ZPYH_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Build and train your custom model and evaluate on the test data."
      ],
      "metadata": {
        "id": "532bbgSUY7EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part D: Compete!\n",
        "To enter the class competition for the best model save out your model as a H5 file and upload it to your assignment repo using the following code block."
      ],
      "metadata": {
        "id": "8PQQIKToY53y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Save your tensorflow model with:\n",
        "model.save('/content/drive/MyDrive/models/yoga-cnn.h5')\n",
        "#TODO: Download your model from Google Drive and upload that saved file to your github assignment repo when you submit your assignment."
      ],
      "metadata": {
        "id": "Ns7XFPYBZAFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
